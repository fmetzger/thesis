%!TEX root = ../../dissertation.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statistical Foundation}

As a final preparation for the evaluation all the necessary statistical tools are briefly introduced in this section with material based on \cite{field2012discovering} and \cite{Knuth:1997:ACP:270146}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Distribution Functions and Fitting}

With a distribution function, also called \gls{CDF}, a monotonous mapping of continous values to a probability can be well represented. It is defined as the probability that a random variable $X$ is less than or equal to a value x, or

\begin{equation}
\phantom{.} F(x) = P(X\leq x)\text{.}
\end{equation}

Mention some basic continous probability distributions relevant to the evaluation: exponential, Gamma, log-normal and Weibull

Sample of real data are generally finite and not continous. Hence, the distribution can only be approximated by an \gls{ECDF} $F_n(x)$ for values $X_1, X_2, ... , X_n$ and 

\begin{equation}
F_n(x) = \frac{\text{number of }X_1, X_2, ... , X_n \leq x}{x}\text{.}
\end{equation}


One of the analysis's goal is to break down the actual measured system to a simplified probability model. This can be conducted by finding probability distributions that match or at least resemble the data's \gls{ECDF}. This can be achieved by using one of several readily available matching methods which rely either on closed formulas or numerical optimization. Two of them are briefly introduced here.

\paragraph{Matching Moments}
Parameters for a selected distribution are estimated by calculating the first and higher moments of the given random variables and solving equations corresponding to the selected distribution.


%http://cran.r-project.org/web/packages/fitdistrplus/fitdistrplus.pdf
 Risk analysis, a quantitative guide \cite[pp.~99-143]{vose2000risk}


\paragraph{Maximum Likelihood}
A fit is found by calculating the log-likelihood of the given random variables for a selected distribution and maximizing the likelihood.

TODO: more details


\paragraph{Eureqa}
In cases where no ``simple'' distribution fit was plausible we attempted to match generic functions to the sample empirical distribution using tools specialized for this case.

TODO: optimization, quoting, short desc





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Statistical Tests}

Null / alternative hypotheses
tests: sum of squares, variance, chi-squared, kolmogorov-smirnov

Variance as a simple test

Generally, tests compare the values observed in an experiment (in our case data obtained from measurements) to expected values following a theoretical distribution. In this case, the tests are used to validate and estimate the quality of the discovered fits to the empirical data.

secondary source for chi and kolmogorov: knuth \cite{Knuth:1997:ACP:270146}


%%
\subsubsection{Visual Inspection}
visual tools: histogram, density, ecdf


%%
\subsubsection{$\chi^2$ Test}


Specifically this means Pearson's chi-square test for independence\cite{doi:10.1080/14786440009463897} and is the oldest known test. It can only be used for discrete count values obtained from independent observations and is compared against a frequency distribution. It is defined as

\begin{equation}
\phantom{.}V=\sum_{i=1}^{k} \frac{(o_i - e_i)^2}{e_i}\text{.}
\end{equation}

This simply calculates the sum of the squared difference between the observed $o_i$ an expected values $e_i$ and adjusts each for their weight. The result can then be compared to the $\chi^2$-distribution with the same degrees of freedom\footnote{The degree of freedom of count experiments is one less than the number of observable categories.} as the test for a given significance level. In most practical cases comparison is conducted against precomputed tables with set significance levels.

Most data collected in this thesis is typically continuous in nature on which this test cannot be used directly. However, data could still be split into a finite number of intervals, as is done when generating a histogram, and then using the intervals as categories for the chi-square test, albeit with a certain loss of precision.


%%
\subsubsection{Kolmogorov-Smirnov Test}

This is where the Kolmogorov-Smirnov Test comes into play. First suggested by Kolmogorov in 1933 \cite{kolmogorov1933sulla} and expanded on by Smirnov in 1939 \cite{smirnov1939estimation} it is defined as

\begin{equation}
	\begin{aligned}
	\phantom{,}K_n^+ = \sqrt{n} \sup_{-\infty < x < + \infty} \left( F_n(x) - F(x) \right), \\
	\phantom{.}K_n^- = \sqrt{n} \sup_{-\infty < x < + \infty} \left( F(x) - F_n(x) \right).
	\end{aligned}
\end{equation}

Once again the results are compared against a precomputed table of values from the Kolmogorov-Smirnov distribution to test the significance of the observed results' deviation from expected values. The advantage is being able to work directly with a measurement's continuous empirical distribution function.

Nowadays, there are also much more powerful statistical tests available. But, as will be explained later, these are not necessary (nor will suffice any better) for our kind of data.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sampling}
Taking randomly selected samples from measurement data does not only simplify handling large sets (working on a set with 2 billion entries is quite problematic) but can even improve statistical significance as long as one keeps in mind, that random sampling error can also be introduced using this.
By selecting entries using a uniform distribution it is ensured that no unintentional sampling bias occurs. The intended evaluation is now applied onto multiple and independently drawn sample groups. If the results of every sample agree then it is also highly likely that the assumption holds for the whole data set.

